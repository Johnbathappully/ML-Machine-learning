{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB92HxSfxLPj"
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy_ZAgmzxLPk"
   },
   "source": [
    "We are going to build a model that predicts if someone who seeks a loan might be a defaulter or a non-defaulter. We have several independent variables like, checking account balance, credit history, purpose, loan amount etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFyI3CAtxLPk"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NjHSaCTxLPl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "#To install xgboost library use - !pip install xgboost \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgm9EEt5xLPo",
    "outputId": "3120b7c3-2035-4c20-9107-5db2ac686ca1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "creditData = pd.read_csv(\"credit.csv\")\n",
    "\n",
    "creditData.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4t1OBSqzxLPr",
    "outputId": "c5db5b2b-a257-4d03-b2cf-ef59e8bed4a2"
   },
   "outputs": [],
   "source": [
    "creditData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditData['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32heA0VFxLPt",
    "outputId": "f1119e0b-fa26-43ea-ba6c-f6d77ae2354c"
   },
   "outputs": [],
   "source": [
    "creditData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uv_-CH_JxLPx",
    "outputId": "36a727e2-731c-4dd1-cb3f-53192e55bd1f"
   },
   "outputs": [],
   "source": [
    "creditData.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVDy-BlVxLPz"
   },
   "source": [
    "Lets convert the columns with an 'object' datatype into categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQVE5XZJxLPz",
    "outputId": "683bab0e-8751-42e5-85e0-b48d80399907"
   },
   "outputs": [],
   "source": [
    "for feature in creditData.columns: # Loop through all columns in the dataframe\n",
    "    if creditData[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
    "        creditData[feature] = pd.Categorical(creditData[feature])# Replace strings with an integer\n",
    "creditData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf4JAhwJxLP2",
    "outputId": "4719ccee-2e82-4174-e740-ebd0d7166d2d"
   },
   "outputs": [],
   "source": [
    "print(creditData.checking_balance.value_counts())\n",
    "print(creditData.credit_history.value_counts())\n",
    "print(creditData.purpose.value_counts())\n",
    "print(creditData.savings_balance.value_counts())\n",
    "print(creditData.employment_duration.value_counts())\n",
    "print(creditData.other_credit.value_counts())\n",
    "print(creditData.housing.value_counts())\n",
    "print(creditData.job.value_counts())\n",
    "print(creditData.phone.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tgft_TIVxLP4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replaceStruct = {\n",
    "                \"checking_balance\":     {\"< 0 DM\": 1, \"1 - 200 DM\": 2 ,\"> 200 DM\": 3 ,\"unknown\":-1},\n",
    "                \"credit_history\": {\"critical\": 1, \"poor\":2 , \"good\": 3, \"very good\": 4,\"perfect\": 5},\n",
    "                 \"savings_balance\": {\"< 100 DM\": 1, \"100 - 500 DM\":2 , \"500 - 1000 DM\": 3, \"> 1000 DM\": 4,\"unknown\": -1},\n",
    "                 \"employment_duration\":     {\"unemployed\": 1, \"< 1 year\": 2 ,\"1 - 4 years\": 3 ,\"4 - 7 years\": 4 ,\"> 7 years\": 5},\n",
    "                \"phone\":     {\"no\": 1, \"yes\": 2 },\n",
    "                #\"job\":     {\"unemployed\": 1, \"unskilled\": 2, \"skilled\": 3, \"management\": 4 },\n",
    "                \"default\":     {\"no\": 0, \"yes\": 1 } \n",
    "                    }\n",
    "oneHotCols=[\"purpose\",\"housing\",\"other_credit\",\"job\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxeyJbJFxLP6",
    "outputId": "ec5bb8e3-b09d-4920-f84b-d85791f37ff4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "creditData=creditData.replace(replaceStruct)\n",
    "creditData=pd.get_dummies(creditData, columns=oneHotCols)\n",
    "creditData.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqTiaxCrxLP8",
    "outputId": "ffb94d79-da96-4923-937a-7171b38eec97"
   },
   "outputs": [],
   "source": [
    "creditData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L-oAMItxLP-"
   },
   "source": [
    "## Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWq265BvxLP_"
   },
   "outputs": [],
   "source": [
    "X = creditData.drop(\"default\" , axis=1)\n",
    "y = creditData.pop(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EIKHRCmxLQB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use some sample functions, to calculate different metrics and plot the confusion matrix. These functions display the output in a structured way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create confusion matrix\n",
    "def make_confusion_matrix(model,y_actual,labels=[1, 0]):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "    y_actual : ground truth  \n",
    "    \n",
    "    '''\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\n",
    "def get_metrics_score(model,flag=True):\n",
    "    '''\n",
    "    model : classifier to predict values of X\n",
    "\n",
    "    '''\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[] \n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    \n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    \n",
    "    train_recall = metrics.recall_score(y_train,pred_train)\n",
    "    test_recall = metrics.recall_score(y_test,pred_test)\n",
    "    \n",
    "    train_precision = metrics.precision_score(y_train,pred_train)\n",
    "    test_precision = metrics.precision_score(y_test,pred_test)\n",
    "    \n",
    "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n",
    "        \n",
    "    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n",
    "    if flag == True: \n",
    "        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n",
    "        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n",
    "        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
    "        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n",
    "        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n",
    "        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n",
    "    \n",
    "    return score_list # returning the list with train and test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "- We are going to build 3 ensemble models here - AdaBoost Classifier, Gradient Boosting Classifier and XGBoost Classifier.\n",
    "- First, let's build these models with default parameters and then use hyperparameter tuning to optimize the model performance.\n",
    "- We will calculate all three metrics - Accuracy, Precision and Recall but the metric of interest here is recall.\n",
    "- `Recall` - It gives the ratio of True positives to Actual positives, so high Recall implies low false negatives, i.e. low chances of predicting a defaulter as non defaulter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(random_state=1)\n",
    "abc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "abc_score=get_metrics_score(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(abc,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state=1)\n",
    "gbc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "gbc_score=get_metrics_score(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(gbc,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=1)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using above defined function to get accuracy, recall and precision on train and test set\n",
    "xgb_score=get_metrics_score(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_confusion_matrix(xgb,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With default parameters:**\n",
    "- AdaBoost classifier has better test accuracy among these 3 models.\n",
    "- GB classifier has least test accuracy and test recall."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Decision_Tree_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
