{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN : Density Based Spatial Clustering of Applications With Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "\n",
    "X, labels_true = make_blobs(n_samples   = 750, \n",
    "                            centers     = centers, \n",
    "                            cluster_std = 0.4,\n",
    "                            random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,0],X[:,1],'.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=.3, min_samples=10).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# labels for all the rows\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated number of clusters: %d'    % n_clusters_)\n",
    "print(\"Silhouette Coefficient      : %0.3f\" % metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot result\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters:**\n",
    "\n",
    "__eps__ : float, optional\n",
    "The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.\n",
    "\n",
    "__min_samples__ : int, optional\n",
    "The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.\n",
    "\n",
    "__metric__ : string, or callable\n",
    "The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by sklearn.metrics.pairwise_distances for its metric parameter. \n",
    "\n",
    "__algorithm__ : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional\n",
    "The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.\n",
    "\n",
    "__leaf_size__ : int, optional (default = 30)\n",
    "Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n",
    "\n",
    "__p__ : float, optional\n",
    "The power of the Minkowski metric to be used to calculate distance between points.\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "__core_sample_indices_ __: array, shape = [n_core_samples] - Indices of core samples.\n",
    "\n",
    "__components_ __: array, shape = [n_core_samples, n_features]\n",
    "Copy of each core sample found by training.\n",
    "\n",
    "__labels_ __: array, shape = [n_samples]\n",
    "Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper parameters\n",
    "eps_space = np.arange(0.1, 5, 0.1)\n",
    "min_samples_space = np.arange(1, 25, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "# Example on counter\n",
    "z = ['blue', 'red', 'blue', 'yellow', 'blue', 'red'] \n",
    "\n",
    "col_count = Counter(z) \n",
    "\n",
    "print(col_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Looping over each combination of hyperparameters\n",
    "\n",
    "dbscan_clusters = []\n",
    "\n",
    "# Starting a tally of total iterations\n",
    "n_iterations = 0\n",
    "\n",
    "for eps_val in eps_space:\n",
    "    for samples_val in min_samples_space:    \n",
    "        \n",
    "        # instantiate DBSCAN\n",
    "        dbscan = DBSCAN(eps = eps_val, min_samples = samples_val)\n",
    "        \n",
    "        # fit & predict\n",
    "        \n",
    "        # fit()) \tPerform DBSCAN clustering from features or distance matrix.\n",
    "        # fit_predict() \tPerforms clustering on X and returns cluster labels.\n",
    "        clusters = dbscan.fit_predict(X = X)\n",
    "        \n",
    "        labels = dbscan.labels_\n",
    "\n",
    "        # Counting the amount of data in each cluster\n",
    "        cluster_count = Counter(clusters)\n",
    "        \n",
    "        # Saving the number of clusters\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "        if n_clusters_ <= 1:\n",
    "            dbscan_clusters.append([eps_val, samples_val, -11])\n",
    "            continue;\n",
    "        \n",
    "        # Increasing the iteration tally with each run of the loop\n",
    "        n_iterations += 1\n",
    "        \n",
    "        sil_score = metrics.silhouette_score(X, labels)\n",
    "        \n",
    "        dbscan_clusters.append([eps_val, samples_val, sil_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(dbscan_clusters)\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['eps', 'min_points', 'silhouette_score'])\n",
    "\n",
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort on the silhouette_score\n",
    "results_df_sorted = results_df.sort_values(['silhouette_score'], ascending=False)\n",
    "results_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the best parameters {eps, min_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DBSCAN\n",
    "dbscan = DBSCAN(eps = 0.4, min_samples = 23)\n",
    "\n",
    "# fit & predict\n",
    "\n",
    "# fit()) \tPerform DBSCAN clustering from features or distance matrix.\n",
    "# fit_predict() \tPerforms clustering on X and returns cluster labels.\n",
    "clusters = dbscan.fit_predict(X = X)\n",
    "\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print(n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    \n",
    "    # Black used for noise.\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, .0001]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    # for the given label and pick the core samples\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], \n",
    "             xy[:, 1], \n",
    "             'o', \n",
    "             markerfacecolor=col,\n",
    "             markeredgecolor='k',\n",
    "             markersize=8)\n",
    "\n",
    "    # for the given label and pick the non core samples (outliers)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], \n",
    "             xy[:, 1], \n",
    "             'o', \n",
    "             markerfacecolor=col,\n",
    "             markeredgecolor='k', \n",
    "             markersize=8)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated number of clusters: %d'    % n_clusters_)\n",
    "print(\"Silhouette Coefficient      : %0.3f\" % metrics.silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Silhouette Score:** \n",
    "\n",
    "The __silhouette score__ is calculated utilizing the mean intra- cluster distance between points, AND the mean nearest-cluster distance. For instance, a cluster with a lot of data points very close to each other (high density) AND is far away from the next nearest cluster (suggesting the cluster is very unique in comparison to the next closest), will have a strong silhouette score. A silhouette score ranges from -1 to 1, with -1 being the worst score possible and 1 being the best score. Silhouette scores of 0 suggest overlapping clusters.\n",
    "\n",
    "**Inertia:**\n",
    "\n",
    "__Inertia__ measures the internal cluster sum of squares (sum of squares is the sum of all residuals). Inertia is utilized to measure how related clusters are amongst themselves, the lower the inertia score the better. HOWEVER, it is important to note that inertia heavily relies on the assumption that the clusters are convex (of spherical shape). DBSCAN does not necessarily divide data into spherical clusters, therefore inertia is not a good metric to use for evaluating DBSCAN models Inertia is more often used in other clustering methods, such as K-means clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
