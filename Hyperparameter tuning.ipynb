{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnxGmBkvq9Rj"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1634811027211,
     "user": {
      "displayName": "Prof. Mukesh Rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02134472667302377777"
     },
     "user_tz": -330
    },
    "id": "UK2k3Mg-q9Rj"
   },
   "outputs": [],
   "source": [
    "# To help with reading and manipulation of data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To impute missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To build a Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# To tune a model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# To get different performance metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# To suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7ck33gHq9Rk",
    "outputId": "39ec123e-dc64-4741-ef22-279552efcae7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Loan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRePnbBq5cJd",
    "outputId": "c89bf037-07c3-4213-c6ec-8075943ae91e"
   },
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fu9Ni_Kk5cJe",
    "outputId": "a201a7b2-1693-4a7c-e435-a28b090842e2"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlCskF3Fq9Rl",
    "outputId": "484f2238-9b83-4ea8-ccb4-d9d9ec07db52"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XbFryla5cJe",
    "outputId": "2c58762b-d38b-4829-ab29-dfa52d6c445e"
   },
   "outputs": [],
   "source": [
    "# checking missing values in the data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g1iA3vkhIzq",
    "outputId": "cbee7bf4-d507-401e-a6fa-51b345edefeb"
   },
   "outputs": [],
   "source": [
    "\n",
    "data[\"region\"] = data[\"region\"].astype(\"category\")\n",
    "data[\"phone_operator\"] = data[\"phone_operator\"].astype(\"category\")\n",
    "data[\"product_type\"] = data[\"product_type\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIQWS7KuhIzr",
    "outputId": "db1357f0-93bd-4dd0-ebac-acb7e5f3ec20"
   },
   "outputs": [],
   "source": [
    "# checking the distribution of the target variable\n",
    "data[\"target\"].value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZFkLVZphIzr"
   },
   "source": [
    "### Splitting the data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82OTq1bvhIzs",
    "outputId": "71ed9f43-1b1a-4559-86a9-19b633b1f80e"
   },
   "outputs": [],
   "source": [
    "# separating the independent and dependent variables\n",
    "X = data.drop([\"target\"], axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "# creating dummy variables\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJM5CvFOhIzs",
    "outputId": "ce058b90-9625-472b-9843-12f5c6c3f5bd"
   },
   "outputs": [],
   "source": [
    "# Splitting data into training, validation and test set:\n",
    "\n",
    "# first we split data into 2 parts, say temporary and test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=5, stratify=y\n",
    ")\n",
    "\n",
    "# then we split the temporary set into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=5, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy7LQSmJ5cJg",
    "outputId": "8da0a126-77d4-4d1f-a1d5-b5c094830256"
   },
   "outputs": [],
   "source": [
    "# Let's impute the missing values\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "\n",
    "# fit the imputer on train data and transform the train data\n",
    "X_train[\"income\"] = imp_median.fit_transform(X_train[[\"income\"]])\n",
    "\n",
    "# transform the validation and test data using the imputer fit on train data\n",
    "X_val[\"income\"] = imp_median.transform(X_val[[\"income\"]])\n",
    "X_test[\"income\"] = imp_median.transform(X_test[[\"income\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCOMaaMGhIzs",
    "outputId": "3d712183-c0a4-49bf-cf47-501d6ac2c9e8"
   },
   "outputs": [],
   "source": [
    "# Checking class balance for whole data, train set, validation set, and test set\n",
    "\n",
    "print(\"Target value ratio in y\")\n",
    "print(y.value_counts(1))\n",
    "print(\"*\" * 80)\n",
    "print(\"Target value ratio in y_train\")\n",
    "print(y_train.value_counts(1))\n",
    "print(\"*\" * 80)\n",
    "print(\"Target value ratio in y_val\")\n",
    "print(y_val.value_counts(1))\n",
    "print(\"*\" * 80)\n",
    "print(\"Target value ratio in y_test\")\n",
    "print(y_test.value_counts(1))\n",
    "print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30jm_OXJat4G"
   },
   "source": [
    "## Model evaluation criterion\n",
    "\n",
    "\n",
    "**What does a bank want?**\n",
    "* A bank wants to minimize the loss - it can face 2 types of losses here: \n",
    "   * Whenever a bank lends money to a customer, they don't return it.\n",
    "   * A bank doesn't lend money to a customer thinking a customer will default but in reality, the customer won't - opportunity loss.\n",
    "\n",
    "**Which loss is greater ?**\n",
    "* Lending to a customer who wouldn't be able to pay back.\n",
    "\n",
    "**Since we want to reduce loan defaults we should use Recall as a metric of model evaluation instead of accuracy.**\n",
    "\n",
    "* Recall - It gives the ratio of True positives to Actual positives, so high Recall implies low false negatives, i.e. low chances of predicting a bad customer as a good customer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6alhuirsj1r"
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NGTaawdat4H"
   },
   "source": [
    "### Let's first build a model with default parameters and see it's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCOiPgwkat4H",
    "outputId": "544ce052-db54-4fb3-fd49-3f4802363c47"
   },
   "outputs": [],
   "source": [
    "# model without hyperparameter tuning\n",
    "rf = RandomForestClassifier(random_state=1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "423qASgLat4H"
   },
   "source": [
    "#### Let's check model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wpmoqh8sat4I",
    "outputId": "c37e18bf-0167-4b9d-b6bd-64a70dd705ba"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on train and validation set\n",
    "print(\"Recall on train and validation set\")\n",
    "print(recall_score(y_train, rf.predict(X_train)))\n",
    "print(recall_score(y_val, rf.predict(X_val)))\n",
    "print(\"\")\n",
    "\n",
    "# Checking Precision score on train and validation set\n",
    "print(\"Precision on train and validation set\")\n",
    "print(precision_score(y_train, rf.predict(X_train)))\n",
    "print(precision_score(y_val, rf.predict(X_val)))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# Checking Accuracy score on train and validation set\n",
    "print(\"Accuracy on train and validation set\")\n",
    "print(accuracy_score(y_train, rf.predict(X_train)))\n",
    "print(accuracy_score(y_val, rf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pagBRSPPat4J"
   },
   "source": [
    "- The model is performing well on the train data but the performance on the validation data is very poor.\n",
    "- Let's see if we can improve it with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EptTvLbuat4J"
   },
   "source": [
    "## Grid Search CV\n",
    "* Hyperparameter tuning is also tricky in the sense that there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model, so we usually resort to experimentation. i.e we'll use Grid search\n",
    "* Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \n",
    "* It is an exhaustive search that is performed on the specific parameter values of a model.\n",
    "* The parameters of the estimator/model used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dlc9_NIat4J"
   },
   "source": [
    "- **How to know the hyperparameters available for an algorithm?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WWA0sEJat4K",
    "outputId": "20318780-a105-4723-a810-6b1db11f6a36"
   },
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcOIKqp4at4K"
   },
   "source": [
    "- We can see the names of hyperparameters available and their default values. \n",
    "- We can choose which ones to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1634811095494,
     "user": {
      "displayName": "Prof. Mukesh Rao",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02134472667302377777"
     },
     "user_tz": -330
    },
    "id": "VnKFyRAwVAPn",
    "outputId": "b84e0df0-1c11-459c-d583-58c4f55a35ad"
   },
   "outputs": [],
   "source": [
    "print(np.arange(0.2, 0.7, 0.1))\n",
    "\n",
    "print(np.arange(5,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m5AtskSat4K"
   },
   "source": [
    "### Let's tune Random forest using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylfOoVr0at4K",
    "outputId": "ee1800b1-618e-4f3a-90fa-3d5c34eb8d24"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Choose the type of classifier. \n",
    "rf1 = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\"n_estimators\": [150,200,250],\n",
    "    \"min_samples_leaf\": np.arange(5, 10),\n",
    "    \"max_features\": np.arange(0.2, 0.7, 0.1),\n",
    "    \"max_samples\": np.arange(0.3, 0.7, 0.1),\n",
    "    \"class_weight\" : ['balanced', 'balanced_subsample'],\n",
    "    \"max_depth\":np.arange(3,4,5),\n",
    "    \"min_impurity_decrease\":[0.001, 0.002, 0.003]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf1, parameters, scoring=acc_scorer, cv=5, n_jobs= -1, verbose = 2)\n",
    "# verbose = 2 tells about the number of fits, which can give an idea of how long will the model take in tuning\n",
    "# n_jobs = -1 so that all CPU cores can be run parallelly to optimize the Search\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Print the best combination of parameters\n",
    "grid_obj.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LpaXjDIat4K"
   },
   "source": [
    "#### Let's check the best CV score, for the obtained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVO8ak5Fat4L",
    "outputId": "881d0886-4817-4d1a-8fa7-db29e1c41e8d"
   },
   "outputs": [],
   "source": [
    "grid_obj.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUvVCFguat4L"
   },
   "source": [
    "#### Let's build a model with obtained best parameters\n",
    "- We are hard coding the hyperparameters separately so that we don't have to run the grid search again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "379_TUHWat4L",
    "outputId": "6c1b901c-1821-4751-c9ac-b6b87f961bb3"
   },
   "outputs": [],
   "source": [
    "# Set the clf to the best combination of parameters\n",
    "rf1_tuned = RandomForestClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    max_features=0.2,\n",
    "    max_samples=0.6000000000000001,\n",
    "    min_samples_leaf=5,\n",
    "    n_estimators=150,\n",
    "    max_depth=3,\n",
    "    random_state=1,\n",
    "    min_impurity_decrease=0.001,\n",
    ")\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "rf1_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGRlwqfuat4M"
   },
   "source": [
    "#### Let's check the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uwkw63nFat4M",
    "outputId": "f57a0146-5766-4c57-9421-1be71fdb3a22"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on train and validation set\n",
    "print(\"Recall on train and validation set\")\n",
    "print(recall_score(y_train, rf1_tuned.predict(X_train)))\n",
    "print(recall_score(y_val, rf1_tuned.predict(X_val)))\n",
    "print(\"\")\n",
    "\n",
    "# Checking precision score on train and validation set\n",
    "print(\"Precision on train and validation set\")\n",
    "print(precision_score(y_train, rf1_tuned.predict(X_train)))\n",
    "print(precision_score(y_val, rf1_tuned.predict(X_val)))\n",
    "print(\"\")\n",
    "\n",
    "# Checking accuracy score on train and validation set\n",
    "print(\"Accuracy on train and validation set\")\n",
    "print(accuracy_score(y_train, rf1_tuned.predict(X_train)))\n",
    "print(accuracy_score(y_val, rf1_tuned.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBnd4W_Fat4N"
   },
   "source": [
    "- We can see improvement in validation performance as compared to the model without hyperparameter tuning\n",
    "- Recall on both training set and validation set is good and is 88% on the validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnujALXkat4O"
   },
   "source": [
    "## Randomized Search CV\n",
    "* Random search is a tuning technique that attempts to compute the optimum values of hyperparameters randomly unlike grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W11Pi8rlat4O"
   },
   "source": [
    "### Let's tune Random forest using Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4Lkjcefat4P",
    "outputId": "e146b028-0de1-46a5-ef44-fe418539dad3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Choose the type of classifier. \n",
    "rf2 = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Grid of parameters to choose from\n",
    "parameters = {\"n_estimators\": [150,200,250],\n",
    "    \"min_samples_leaf\": np.arange(5, 10),\n",
    "    \"max_features\": np.arange(0.2, 0.7, 0.1), \n",
    "    \"max_samples\": np.arange(0.3, 0.7, 0.1),\n",
    "    \"max_depth\":np.arange(3,4,5),\n",
    "    \"class_weight\" : ['balanced', 'balanced_subsample'],\n",
    "    \"min_impurity_decrease\":[0.001, 0.002, 0.003]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "# Run the random search\n",
    "grid_obj = RandomizedSearchCV(rf2, parameters,n_iter=30, scoring=acc_scorer,cv=5, random_state = 1, n_jobs = -1, verbose = 2)\n",
    "# using n_iter = 30, so randomized search will try 30 different combinations of hyperparameters\n",
    "# by default, n_iter = 10\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Print the best combination of parameters\n",
    "grid_obj.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC3IB5JAat4P"
   },
   "source": [
    "#### Let's check the best CV score, for the obtained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HZAnjxfat4P",
    "outputId": "99fe8701-ae44-4905-c2f6-0c503aee1884"
   },
   "outputs": [],
   "source": [
    "grid_obj.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCFHwl5xat4P"
   },
   "source": [
    "#### Let's build a model with obtained best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8TWo6tTat4Q",
    "outputId": "d33dcf4c-0d32-4b82-ec89-9a4263d312a6"
   },
   "outputs": [],
   "source": [
    "# Set the clf to the best combination of parameters\n",
    "rf2_tuned = RandomForestClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    max_features=0.2,\n",
    "    max_samples=0.5,\n",
    "    min_samples_leaf=5,\n",
    "    n_estimators=150,\n",
    "    random_state=1,\n",
    "    max_depth=3,\n",
    "    min_impurity_decrease=0.003,\n",
    ")\n",
    "\n",
    "# Fit the best algorithm to the data.\n",
    "rf2_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZZ8N0M4at4Q"
   },
   "source": [
    "- Different results from the grid and the random search\n",
    "- Randomised search might give better results than grid search for the same parameter grid because of the use of cross-validation as fold varies the scores also vary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSEY5KJ-at4Q"
   },
   "source": [
    "#### Let's check the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgGjmHayat4Q",
    "outputId": "2c276e16-5b6f-4e04-c3d8-16878a0f1670"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on train and validation set\n",
    "print(\"Recall on train and validation set\")\n",
    "print(recall_score(y_train, rf2_tuned.predict(X_train)))\n",
    "print(recall_score(y_val, rf2_tuned.predict(X_val)))\n",
    "print(\"\")\n",
    "print(\"Precision on train and validation set\")\n",
    "# Checking precision score on train and validation set\n",
    "print(precision_score(y_train, rf2_tuned.predict(X_train)))\n",
    "print(precision_score(y_val, rf2_tuned.predict(X_val)))\n",
    "print(\"\")\n",
    "print(\"Accuracy on train and validation set\")\n",
    "# Checking accuracy score on train and validation set\n",
    "print(accuracy_score(y_train, rf2_tuned.predict(X_train)))\n",
    "print(accuracy_score(y_val, rf2_tuned.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebNJl1ZKat4R"
   },
   "source": [
    "- The model is performing better than model with default parameters and the performance is similar to the model we received with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVeUUOVjat4R"
   },
   "source": [
    "#### Choose a best model and predict the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmBsx6I3at4R",
    "outputId": "21dff7fe-483f-4663-cdcc-7a3d0b86dd9f"
   },
   "outputs": [],
   "source": [
    "model = rf1_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wazimV7oat4R",
    "outputId": "5c5606f7-476a-4844-ca55-9079a98b92ab"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on test set\n",
    "print(\"Recall on test set\")\n",
    "print(recall_score(y_test, model.predict(X_test)))\n",
    "print(\"\")\n",
    "\n",
    "# Checking precision score on test set\n",
    "print(\"Precision on test set\")\n",
    "print(precision_score(y_test, model.predict(X_test)))\n",
    "print(\"\")\n",
    "\n",
    "# Checking accuracy score on test set\n",
    "print(\"Accuracy on test set\")\n",
    "print(accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UO2wLc07at4R"
   },
   "source": [
    "- The performance is close to one we observed in the validation set, so there is no overfitting"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hyperparameter tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
